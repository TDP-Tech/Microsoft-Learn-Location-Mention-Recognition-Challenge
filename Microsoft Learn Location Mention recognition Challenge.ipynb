{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52df4a3",
   "metadata": {},
   "source": [
    "### IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0452cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a17bd",
   "metadata": {},
   "source": [
    "### LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b304a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(r\"E:\\AI,ML,NLP PROJECTS\\Microsoft Learn Location Mention Recognition Challenge\\Train.csv\")\n",
    "test_data = pd.read_csv(r\"E:\\AI,ML,NLP PROJECTS\\Microsoft Learn Location Mention Recognition Challenge\\Test.csv\")\n",
    "sample_submission = pd.read_csv(r\"E:\\AI,ML,NLP PROJECTS\\Microsoft Learn Location Mention Recognition Challenge\\SampleSubmission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d844d424",
   "metadata": {},
   "source": [
    "### EXPLORING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc76db56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 tweet_id                                               text  \\\n",
      "0  ID_1001136212718088192                                                NaN   \n",
      "1  ID_1001136696589631488  Flash floods struck a Maryland city on Sunday,...   \n",
      "2  ID_1001136950345109504  State of emergency declared for Maryland flood...   \n",
      "3  ID_1001137334056833024  Other parts of Maryland also saw significant d...   \n",
      "4  ID_1001138374923579392  Catastrophic Flooding Slams Ellicott City, Mar...   \n",
      "\n",
      "                 location  \n",
      "0            EllicottCity  \n",
      "1                Maryland  \n",
      "2                Maryland  \n",
      "3      Baltimore Maryland  \n",
      "4  Ellicott City Maryland  \n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1dbe57a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 tweet_id                                               text\n",
      "0  ID_1001154804658286592  What is happening to the infrastructure in New...\n",
      "1  ID_1001155505459486720  SOLDER MISSING IN FLOOD.. PRAY FOR EDDISON HER...\n",
      "2  ID_1001155756371136512  RT @TIME: Police searching for missing person ...\n",
      "3  ID_1001159445194399744  Flash Flood Tears Through Maryland Town For Se...\n",
      "4  ID_1001164907587538944  Ellicott City #FLOODING Pictures: Maryland Gov...\n"
     ]
    }
   ],
   "source": [
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aafa8242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 tweet_id  location\n",
      "0  ID_1001154804658286592       NaN\n",
      "1  ID_1001155505459486720       NaN\n",
      "2  ID_1001155756371136512       NaN\n",
      "3  ID_1001159445194399744       NaN\n",
      "4  ID_1001164907587538944       NaN\n"
     ]
    }
   ],
   "source": [
    "print(sample_submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a3485",
   "metadata": {},
   "source": [
    "### UNDERSTANDING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a83131",
   "metadata": {},
   "source": [
    "**Train.csv:** This file should contain the microblogging posts along with the corresponding location mentions.\n",
    "\n",
    "**Test.csv:** This file will have the microblogging posts but without the location mentions, which you need to predict.\n",
    "\n",
    "**SampleSubmission.csv:** This shows the format in which you need to submit your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb266469",
   "metadata": {},
   "source": [
    "### BASIC STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b25e6435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training data statistics\n",
      "\n",
      "                      tweet_id  \\\n",
      "count                    73072   \n",
      "unique                   73072   \n",
      "top     ID_1001136212718088192   \n",
      "freq                         1   \n",
      "\n",
      "                                                     text    location  \n",
      "count                                               16448       43460  \n",
      "unique                                              16448        7730  \n",
      "top     Flash floods struck a Maryland city on Sunday,...  California  \n",
      "freq                                                    1        4224  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining data statistics\\n\")\n",
    "print(train_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8220aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns in Train.csv:\n",
      "Index(['tweet_id', 'text', 'location'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check the columns in the dataset\n",
    "print(\"\\nColumns in Train.csv:\")\n",
    "print(train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4057262b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 73072 entries, 0 to 73071\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   tweet_id  73072 non-null  object\n",
      " 1   text      16448 non-null  object\n",
      " 2   location  43460 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"\\nBasic Information:\")\n",
    "print(train_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08b99926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of unique locations in the training data\n",
      "['EllicottCity' 'Maryland' 'Baltimore Maryland' ... 'LasVega'\n",
      " 'Israels Mexico City Thailand' 'Ecuador Mexico']\n"
     ]
    }
   ],
   "source": [
    "# check the number of unique locations in the training data\n",
    "print(\"\\nNumber of unique locations in the training data\")\n",
    "print(train_data['location'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f93cb",
   "metadata": {},
   "source": [
    "### CHECKING FOR MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f187b326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length of a training data:\n",
      "\n",
      "73072\n",
      "\n",
      "Missing values in training data:\n",
      "\n",
      "tweet_id        0\n",
      "text        56624\n",
      "location    29612\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLength of a training data:\\n\")\n",
    "print(len(train_data))\n",
    "print(\"\\nMissing values in training data:\\n\")\n",
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecbf5ca",
   "metadata": {},
   "source": [
    "### TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1844de1",
   "metadata": {},
   "source": [
    "Text preprocessing is a crucial part to ensure the model can effectively understand the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7207fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text to lower case\n",
    "train_data['text'] = train_data['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b511d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all text entries are strings\n",
    "train_data['text'] = train_data['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24905c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaN values with empty string\n",
    "train_data['text'] = train_data['text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "431a28c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary symbols\n",
    "import re\n",
    "\n",
    "#remove urls, mentions, hashtags, and special characters\n",
    "train_data['text'] = train_data['text'].apply(lambda x: re.sub(r'http\\S+|www\\S+|@\\S+|#\\S+', '', x))\n",
    "train_data['text'] = train_data['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x)) # remove punctuation\n",
    "train_data['text'] = train_data['text'].apply(lambda x: re.sub(r'\\d+', '', x)) # remove numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671bf1e6",
   "metadata": {},
   "source": [
    "### TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff4030",
   "metadata": {},
   "source": [
    "Tokenization involves breaking down the text into individual words (tokens). \n",
    "\n",
    "This step is essential for further processing and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98caec7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28762ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "train_data['tokens'] = train_data['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "201164fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized text examples\n",
      "                                                text  \\\n",
      "0                                                nan   \n",
      "1  flash floods struck a maryland city on sunday ...   \n",
      "2  state of emergency declared for maryland flood...   \n",
      "3  other parts of maryland also saw significant d...   \n",
      "4  catastrophic flooding slams ellicott city mary...   \n",
      "\n",
      "                                              tokens  \n",
      "0                                              [nan]  \n",
      "1  [flash, floods, struck, a, maryland, city, on,...  \n",
      "2  [state, of, emergency, declared, for, maryland...  \n",
      "3  [other, parts, of, maryland, also, saw, signif...  \n",
      "4  [catastrophic, flooding, slams, ellicott, city...  \n"
     ]
    }
   ],
   "source": [
    "# Display a few examples of tokenized text\n",
    "print('\\nTokenized text examples')\n",
    "print(train_data[['text', 'tokens']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a958edbe",
   "metadata": {},
   "source": [
    "### HANDLE ABBREVIATIONS AND COMMON TERMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c69a8eb",
   "metadata": {},
   "source": [
    "Location mentions might include abbreviations, for example \"NYC\" for New York City, \"NY\" for New York and so on.\n",
    "\n",
    "Handling these abbreviations is crucial for accurate location recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bf8364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for handling abbreviations\n",
    "\n",
    "abbreviations = {\n",
    "    \"nyc\": \"new york city\",\n",
    "    \"la\": \"los angeles\",\n",
    "    \"sf\": \"san ransisco\",\n",
    "    \"dc\": \"washington dc\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94f81ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_abbreviations(text, abbreviation_dict):\n",
    "    words = text.split()\n",
    "    replaced_text = ' '.join([abbreviation_dict.get(word, word) for word in words])\n",
    "    return replaced_text\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(lambda x: replace_abbreviations(x, abbreviations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31970389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text after handling abbreviations:\n",
      "0                                                  nan\n",
      "1    flash floods struck a maryland city on sunday ...\n",
      "2    state of emergency declared for maryland flood...\n",
      "3    other parts of maryland also saw significant d...\n",
      "4    catastrophic flooding slams ellicott city mary...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Display the modified text after abbreviation replaced\n",
    "\n",
    "print(\"\\nText after handling abbreviations:\")\n",
    "print(train_data['text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d9a38",
   "metadata": {},
   "source": [
    "### SAVE THE PROCESSED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0ce3314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to save the preprocessed data\n",
    "processed_file_path = r\"E:\\AI,ML,NLP PROJECTS\\Microsoft Learn Location Mention Recognition Challenge\\Processed_Train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85f48778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data to a new CSV file\n",
    "train_data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd90ecc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed Train Data\n",
      "                 tweet_id                                               text  \\\n",
      "0  ID_1001136212718088192                                                nan   \n",
      "1  ID_1001136696589631488  flash floods struck a maryland city on sunday ...   \n",
      "2  ID_1001136950345109504  state of emergency declared for maryland flood...   \n",
      "3  ID_1001137334056833024  other parts of maryland also saw significant d...   \n",
      "4  ID_1001138374923579392  catastrophic flooding slams ellicott city mary...   \n",
      "\n",
      "                 location                                             tokens  \n",
      "0            EllicottCity                                              [nan]  \n",
      "1                Maryland  [flash, floods, struck, a, maryland, city, on,...  \n",
      "2                Maryland  [state, of, emergency, declared, for, maryland...  \n",
      "3      Baltimore Maryland  [other, parts, of, maryland, also, saw, signif...  \n",
      "4  Ellicott City Maryland  [catastrophic, flooding, slams, ellicott, city...  \n"
     ]
    }
   ],
   "source": [
    "# Display the first five rows of the processed data\n",
    "print(\"\\nProcessed Train Data\")\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e896bea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to E:\\AI,ML,NLP PROJECTS\\Microsoft Learn Location Mention Recognition Challenge\\Processed_Train.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processed data saved to {processed_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe835e",
   "metadata": {},
   "source": [
    "### Step 1: Vectorizing the Text Data using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51f79f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000) # adjust according to your needs\n",
    "\n",
    "# Fit and Transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['text'])\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e533421d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix for training data: (73072, 5000)\n",
      "TF-IDF matrix for test data: (2942, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Display the shape of the resulting matrices\n",
    "print(\"TF-IDF matrix for training data:\", X_train_tfidf.shape)\n",
    "print(\"TF-IDF matrix for test data:\", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd0779",
   "metadata": {},
   "source": [
    "### Step 2.1: Process the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d0742ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# Fill NaN values with an empty string\n",
    "train_data['location'] = train_data['location'].fillna('')\n",
    "\n",
    "# Split the location mention into a list\n",
    "train_data['location_list'] = train_data['location'].apply(lambda x: x.split())\n",
    "\n",
    "# Initialize the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit and transform the location list to a binary matrix\n",
    "y_train = mlb.fit_transform(train_data['location_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a948de5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the target variable matrix: (73072, 4285)\n"
     ]
    }
   ],
   "source": [
    "# Display the shape of the resulting matrix\n",
    "print(\"Shape of the target variable matrix:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40de465",
   "metadata": {},
   "source": [
    "### MODEL SELECTION AND TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7da208d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneVsRestClassifier(estimator=LogisticRegression(solver=&#x27;liblinear&#x27;))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneVsRestClassifier</label><div class=\"sk-toggleable__content\"><pre>OneVsRestClassifier(estimator=LogisticRegression(solver=&#x27;liblinear&#x27;))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(solver='liblinear'))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr_model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Use OneVsRestClassifier for multi-label classification\n",
    "model = OneVsRestClassifier(lr_model)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bcf74c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.20\n"
     ]
    }
   ],
   "source": [
    "# Display the training accuracy\n",
    "training_accuracy = model.score(X_train_tfidf, y_train)\n",
    "print(\"Training accuracy: {:.2f}\".format(training_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37694f85",
   "metadata": {},
   "source": [
    "### Step 3.2: Predicting on Test Data\n",
    "\n",
    "Predict the locations after training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ad1536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the locations for the teX_test_tfidfst_tfidfst_tfidfst_tfidfst_tfidfst_tfidfst_tfidfst_tfidfst_tfidfata\n",
    "y_test_prediction = model.predict(X_test_tfidf)\n",
    "\n",
    "# Convert the preinverse_transform back to the original location format\n",
    "predicted_locations = mlb.inverse_transform(y_test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e2cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepair the prediction file for submission\n",
    "submission_df = test_data[['ID']].copy()\n",
    "submission_dfission_dfission_df['locations'] = [' '.join(loc) for loc in predicted_locations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d06e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the submission file\n",
    "submission_df.to_csv(r\"E:\\AI,ML,NLP PROJECTS\\Microsoft Learn Location Mention Recognition Challenge\\Submission.csv\", index=False)\n",
    "print(\"Submission file created and saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
